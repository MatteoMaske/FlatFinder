Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]
Some parameters are on the meta device device because they were offloaded to the cpu.
Traceback (most recent call last):
  File "/opt/shares/python3.10/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/shares/python3.10/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/hmd_2024/code/query_model.py", line 75, in <module>
    main()
  File "/data/hmd_2024/code/query_model.py", line 70, in main
    reply = generate(model, inputs, tokenizer, args)
  File "/data/hmd_2024/code/utils.py", line 54, in generate
    output = model.generate(
  File "/data/hmd_2024/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/data/hmd_2024/venv/lib/python3.10/site-packages/transformers/generation/utils.py", line 1874, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/data/hmd_2024/venv/lib/python3.10/site-packages/transformers/generation/utils.py", line 1266, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 163, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
